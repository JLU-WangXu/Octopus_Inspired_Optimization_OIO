#!/usr/bin/env python3
"""
CEC2022åŸºå‡†æµ‹è¯•è„šæœ¬ - OIO vs 15ç§åŸºçº¿ç®—æ³•
æµ‹è¯•F1~F12å‡½æ•°ï¼Œ16çº¿ç¨‹å¹¶è¡Œï¼Œ20000æ¬¡è¯„ä¼°ï¼Œé‡å¤10æ¬¡
åŒ…å«å¯è§†åŒ–åˆ†æå’Œç»Ÿè®¡æ£€éªŒ
"""

import time
import numpy as np
import pandas as pd
import gc
import tracemalloc
from typing import Callable, Tuple, Optional
from concurrent.futures import ThreadPoolExecutor, as_completed
import sys
import os
import matplotlib.pyplot as plt
import matplotlib.font_manager as fm
from scipy.stats import wilcoxon, mannwhitneyu
import seaborn as sns
from pathlib import Path
import json
import pickle
from datetime import datetime

# æ·»åŠ çˆ¶ç›®å½•åˆ°è·¯å¾„ä»¥å¯¼å…¥OIOå’ŒåŸºçº¿ç®—æ³•
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

try:
    from opfunu.cec_based.cec2022 import (
        F12022, F22022, F32022, F42022, F52022, F62022,
        F72022, F82022, F92022, F102022, F112022, F122022
    )
    print("âœ… æˆåŠŸå¯¼å…¥opfunu CEC2022å‡½æ•°")
except ImportError:
    print("âŒ é”™è¯¯: éœ€è¦å®‰è£…opfunuåº“")
    print("è¯·è¿è¡Œ: pip install opfunu")
    sys.exit(1)

from OIO import ChampionOIOAlgorithm
from baseline import ALGORITHMS

# ============================================================================
# æ—©åœæœºåˆ¶ - å¼‚å¸¸ä¸­æ–­æ¨¡å¼
# ============================================================================

class TargetPrecisionReached(Exception):
    """ç”¨äºåœ¨è¾¾åˆ°ç›®æ ‡ç²¾åº¦æ—¶ä¸­æ–­ç®—æ³•çš„è‡ªå®šä¹‰å¼‚å¸¸"""
    def __init__(self, message, fitness, fes):
        super().__init__(message)
        self.fitness = fitness
        self.fes = fes

class InterruptibleCostFunc:
    """ä¸€ä¸ªå¯ä¸­æ–­çš„æˆæœ¬å‡½æ•°åŒ…è£…å™¨ï¼Œå¹¶è®°å½•å…¨å±€æ”¶æ•›å†å²"""
    def __init__(self, original_cost_func, max_evaluations, success_threshold=1e-8):
        self.original_cost_func = original_cost_func
        self.max_evaluations = max_evaluations
        self.success_threshold = success_threshold
        self.evaluation_count = 0
        self.best_fitness = float('inf')

        # --- ä¿®æ”¹ï¼šæ”¶æ•›å†å²è®°å½• (FES, fitness) å…ƒç»„ ---
        self.convergence_history = []  # ç°åœ¨å­˜å‚¨ (FES, fitness) å…ƒç»„
        # ä¸ºäº†é¿å…è®°å½•è¿‡å¤šæ•°æ®ç‚¹ï¼Œæˆ‘ä»¬å¯ä»¥è®¾ç½®ä¸€ä¸ªè®°å½•é—´éš”
        # æ¯è¯„ä¼° record_interval æ¬¡ï¼Œè®°å½•ä¸€æ¬¡å½“å‰çš„æœ€ä¼˜å€¼
        self.record_interval = max(1, max_evaluations // 500)  # è®°å½•500ä¸ªç‚¹
        # --- ä¿®æ”¹ç»“æŸ ---

    def __call__(self, x):
        # æ£€æŸ¥1ï¼šæ˜¯å¦å·²è¾¾åˆ°æœ€å¤§è¯„ä¼°æ¬¡æ•°
        if self.evaluation_count >= self.max_evaluations:
            # æŠ›å‡ºå¼‚å¸¸ï¼Œè¡¨ç¤ºè¯„ä¼°æ¬¡æ•°è€—å°½
            raise RuntimeError("Maximum evaluations reached")

        self.evaluation_count += 1
        fitness = self.original_cost_func(x)

        new_best_found = False
        if fitness < self.best_fitness:
            self.best_fitness = fitness
            new_best_found = True

        # --- ä¿®æ”¹ï¼šè®°å½•æ”¶æ•›å†å² (FES, fitness) å…ƒç»„ ---
        # æ¯éš”ä¸€å®šé—´éš”ï¼Œæˆ–è€…æ‰¾åˆ°äº†æ–°çš„æœ€ä¼˜å€¼æ—¶ï¼Œè®°å½•ä¸€æ¬¡
        if new_best_found or self.evaluation_count % self.record_interval == 0:
            # è®°å½•å½“å‰çš„è¯„ä¼°æ¬¡æ•°å’Œå¯¹åº”çš„æœ€ä¼˜å€¼
            self.convergence_history.append((self.evaluation_count, self.best_fitness))
        # --- ä¿®æ”¹ç»“æŸ ---

        # æ£€æŸ¥2ï¼šæ˜¯å¦è¾¾åˆ°ç›®æ ‡ç²¾åº¦
        # æ³¨æ„ï¼šCEC2022çš„ç†è®ºæœ€ä¼˜å€¼æ˜¯0
        error = fitness - 0.0
        if error < self.success_threshold:
            # --- ä¿®æ”¹ï¼šåœ¨æŠ›å‡ºå¼‚å¸¸å‰ï¼Œç¡®ä¿æœ€åçš„æœ€ä¼˜å€¼è¢«è®°å½• ---
            if not self.convergence_history or self.convergence_history[-1][1] != self.best_fitness:
                self.convergence_history.append((self.evaluation_count, self.best_fitness))
            # --- ä¿®æ”¹ç»“æŸ ---
            # æŠ›å‡ºè‡ªå®šä¹‰å¼‚å¸¸ï¼Œæºå¸¦æˆåŠŸä¿¡æ¯
            raise TargetPrecisionReached(
                "Target precision reached",
                fitness=self.best_fitness,
                fes=self.evaluation_count
            )

        return fitness

def get_memory_usage():
    """è·å–å½“å‰å†…å­˜ä½¿ç”¨é‡(MB) - ä½¿ç”¨tracemalloc"""
    try:
        current, _ = tracemalloc.get_traced_memory()
        return current / 1024 / 1024  # è½¬æ¢ä¸ºMB
    except:
        return 0.0

# ============================================================================
# æ•°æ®ç®¡ç†æ¨¡å—
# ============================================================================

class DataManager:
    """æ•°æ®ç®¡ç†å™¨ - è´Ÿè´£ä¿å­˜å’ŒåŠ è½½å®éªŒæ•°æ®"""

    def __init__(self, data_dir='experiment_data'):
        """åˆå§‹åŒ–æ•°æ®ç®¡ç†å™¨

        Args:
            data_dir: æ•°æ®å­˜å‚¨ç›®å½•
        """
        self.data_dir = Path(data_dir)
        self.data_dir.mkdir(exist_ok=True)

        # åˆ›å»ºå­ç›®å½•
        self.convergence_dir = self.data_dir / 'convergence_data'
        self.results_dir = self.data_dir / 'results_data'
        self.plots_dir = self.data_dir / 'plot_data'

        for dir_path in [self.convergence_dir, self.results_dir, self.plots_dir]:
            dir_path.mkdir(exist_ok=True)

    def save_convergence_data(self, func_name, algorithm_name, convergence_history, run_id=None):
        """ä¿å­˜æ”¶æ•›æ›²çº¿æ•°æ®

        Args:
            func_name: å‡½æ•°åç§° (å¦‚ 'F1')
            algorithm_name: ç®—æ³•åç§° (å¦‚ 'OIO')
            convergence_history: æ”¶æ•›å†å²æ•°æ® (list)
            run_id: è¿è¡ŒIDï¼Œå¦‚æœä¸ºNoneåˆ™è‡ªåŠ¨ç”Ÿæˆæ—¶é—´æˆ³
        """
        if run_id is None:
            run_id = datetime.now().strftime("%Y%m%d_%H%M%S")

        filename = f"{func_name}_{algorithm_name}_{run_id}.json"
        filepath = self.convergence_dir / filename

        data = {
            'function': func_name,
            'algorithm': algorithm_name,
            'run_id': run_id,
            'timestamp': datetime.now().isoformat(),
            'convergence_history': convergence_history
        }

        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(data, f, indent=2, ensure_ascii=False)

    def load_convergence_data(self, func_name, algorithm_name, run_id=None):
        """åŠ è½½æ”¶æ•›æ›²çº¿æ•°æ®

        Args:
            func_name: å‡½æ•°åç§°
            algorithm_name: ç®—æ³•åç§°
            run_id: è¿è¡ŒIDï¼Œå¦‚æœä¸ºNoneåˆ™åŠ è½½æœ€æ–°çš„æ•°æ®

        Returns:
            convergence_history: æ”¶æ•›å†å²æ•°æ®ï¼Œå¦‚æœæœªæ‰¾åˆ°è¿”å›None
        """
        if run_id is not None:
            filename = f"{func_name}_{algorithm_name}_{run_id}.json"
            filepath = self.convergence_dir / filename
            if filepath.exists():
                with open(filepath, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                return data['convergence_history']
        else:
            # æŸ¥æ‰¾æœ€æ–°çš„æ•°æ®æ–‡ä»¶
            pattern = f"{func_name}_{algorithm_name}_*.json"
            files = list(self.convergence_dir.glob(pattern))
            if files:
                # æŒ‰ä¿®æ”¹æ—¶é—´æ’åºï¼Œå–æœ€æ–°çš„
                latest_file = max(files, key=lambda x: x.stat().st_mtime)
                with open(latest_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                return data['convergence_history']

        return None

    def save_experiment_results(self, all_results, experiment_config, run_id=None):
        """ä¿å­˜å®Œæ•´çš„å®éªŒç»“æœ

        Args:
            all_results: æ‰€æœ‰å®éªŒç»“æœ
            experiment_config: å®éªŒé…ç½®ä¿¡æ¯
            run_id: è¿è¡ŒID
        """
        if run_id is None:
            run_id = datetime.now().strftime("%Y%m%d_%H%M%S")

        filename = f"experiment_results_{run_id}.pkl"
        filepath = self.results_dir / filename

        data = {
            'results': all_results,
            'config': experiment_config,
            'run_id': run_id,
            'timestamp': datetime.now().isoformat()
        }

        with open(filepath, 'wb') as f:
            pickle.dump(data, f)

        # åŒæ—¶ä¿å­˜ä¸€ä¸ªJSONç‰ˆæœ¬ï¼ˆä¸åŒ…å«å¤æ‚å¯¹è±¡ï¼‰
        json_filename = f"experiment_results_{run_id}.json"
        json_filepath = self.results_dir / json_filename

        # è½¬æ¢ç»“æœä¸ºJSONå¯åºåˆ—åŒ–æ ¼å¼
        json_results = self._convert_results_to_json(all_results)
        json_data = {
            'results': json_results,
            'config': experiment_config,
            'run_id': run_id,
            'timestamp': datetime.now().isoformat()
        }

        with open(json_filepath, 'w', encoding='utf-8') as f:
            json.dump(json_data, f, indent=2, ensure_ascii=False)

    def load_experiment_results(self, run_id=None):
        """åŠ è½½å®éªŒç»“æœ

        Args:
            run_id: è¿è¡ŒIDï¼Œå¦‚æœä¸ºNoneåˆ™åŠ è½½æœ€æ–°çš„ç»“æœ

        Returns:
            (all_results, experiment_config): å®éªŒç»“æœå’Œé…ç½®
        """
        if run_id is not None:
            filename = f"experiment_results_{run_id}.pkl"
            filepath = self.results_dir / filename
            if filepath.exists():
                with open(filepath, 'rb') as f:
                    data = pickle.load(f)
                return data['results'], data['config']
        else:
            # æŸ¥æ‰¾æœ€æ–°çš„ç»“æœæ–‡ä»¶
            pattern = "experiment_results_*.pkl"
            files = list(self.results_dir.glob(pattern))
            if files:
                latest_file = max(files, key=lambda x: x.stat().st_mtime)
                with open(latest_file, 'rb') as f:
                    data = pickle.load(f)
                return data['results'], data['config']

        return None, None

    def _convert_results_to_json(self, all_results):
        """å°†ç»“æœè½¬æ¢ä¸ºJSONå¯åºåˆ—åŒ–æ ¼å¼"""
        json_results = {}
        for func_name, func_results in all_results.items():
            json_results[func_name] = {}
            for alg_name, alg_result in func_results.items():
                if 'error' in alg_result:
                    json_results[func_name][alg_name] = {'error': alg_result['error']}
                else:
                    json_results[func_name][alg_name] = {
                        'fitness_mean': float(alg_result.get('fitness_mean', 0)),
                        'fitness_std': float(alg_result.get('fitness_std', 0)),
                        'fitness_best': float(alg_result.get('fitness_best', 0)),
                        'fitness_worst': float(alg_result.get('fitness_worst', 0)),
                        'time_mean': float(alg_result.get('time_mean', 0)),
                        'success_count': int(alg_result.get('success_count', 0)),
                        'total_runs': int(alg_result.get('total_runs', 0)),
                        'all_fitness': [float(x) for x in alg_result.get('all_fitness', [])],
                        'all_times': [float(x) for x in alg_result.get('all_times', [])],
                        'convergence_histories': alg_result.get('convergence_histories', [])
                    }
        return json_results

    def list_available_data(self):
        """åˆ—å‡ºå¯ç”¨çš„æ•°æ®æ–‡ä»¶"""
        print(f"\nğŸ“ å¯ç”¨çš„å®éªŒæ•°æ® (å­˜å‚¨åœ¨ {self.data_dir}):")

        # åˆ—å‡ºå®éªŒç»“æœæ–‡ä»¶
        result_files = list(self.results_dir.glob("experiment_results_*.pkl"))
        if result_files:
            print(f"\nğŸ”¬ å®éªŒç»“æœæ–‡ä»¶ ({len(result_files)} ä¸ª):")
            for file in sorted(result_files, key=lambda x: x.stat().st_mtime, reverse=True):
                mtime = datetime.fromtimestamp(file.stat().st_mtime)
                print(f"  - {file.name} (ä¿®æ”¹æ—¶é—´: {mtime.strftime('%Y-%m-%d %H:%M:%S')})")

        # åˆ—å‡ºæ”¶æ•›æ•°æ®æ–‡ä»¶
        conv_files = list(self.convergence_dir.glob("*.json"))
        if conv_files:
            print(f"\nğŸ“ˆ æ”¶æ•›æ•°æ®æ–‡ä»¶ ({len(conv_files)} ä¸ª):")
            # æŒ‰å‡½æ•°å’Œç®—æ³•åˆ†ç»„
            by_func_alg = {}
            for file in conv_files:
                parts = file.stem.split('_')
                if len(parts) >= 3:
                    func_name = parts[0]
                    alg_name = parts[1]
                    key = f"{func_name}_{alg_name}"
                    if key not in by_func_alg:
                        by_func_alg[key] = []
                    by_func_alg[key].append(file)

            for key, files in sorted(by_func_alg.items()):
                print(f"  - {key}: {len(files)} ä¸ªè¿è¡Œè®°å½•")

    def save_plot_data(self, plot_type, data, filename=None):
        """ä¿å­˜ç»˜å›¾æ•°æ®

        Args:
            plot_type: ç»˜å›¾ç±»å‹ ('histogram', 'convergence', 'statistical')
            data: ç»˜å›¾æ•°æ®
            filename: æ–‡ä»¶åï¼Œå¦‚æœä¸ºNoneåˆ™è‡ªåŠ¨ç”Ÿæˆ
        """
        if filename is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"{plot_type}_data_{timestamp}.json"

        filepath = self.plots_dir / filename

        plot_data = {
            'plot_type': plot_type,
            'data': data,
            'timestamp': datetime.now().isoformat()
        }

        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(plot_data, f, indent=2, ensure_ascii=False)

        return filepath

    def load_plot_data(self, plot_type, filename=None):
        """åŠ è½½ç»˜å›¾æ•°æ®

        Args:
            plot_type: ç»˜å›¾ç±»å‹
            filename: æ–‡ä»¶åï¼Œå¦‚æœä¸ºNoneåˆ™åŠ è½½æœ€æ–°çš„

        Returns:
            ç»˜å›¾æ•°æ®ï¼Œå¦‚æœæœªæ‰¾åˆ°è¿”å›None
        """
        if filename is not None:
            filepath = self.plots_dir / filename
            if filepath.exists():
                with open(filepath, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                return data['data']
        else:
            # æŸ¥æ‰¾æœ€æ–°çš„ç»˜å›¾æ•°æ®
            pattern = f"{plot_type}_data_*.json"
            files = list(self.plots_dir.glob(pattern))
            if files:
                latest_file = max(files, key=lambda x: x.stat().st_mtime)
                with open(latest_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                return data['data']

        return None

# ============================================================================
# CEC2022å‡½æ•°é€‚é…å™¨
# ============================================================================

class CEC2022FunctionAdapter:
    """CEC2022å‡½æ•°é€‚é…å™¨ï¼Œå°†è¿ç»­ä¼˜åŒ–å‡½æ•°é€‚é…ä¸ºåŸºçº¿ç®—æ³•æ¥å£"""

    def __init__(self, cec_func, dim):
        self.cec_func = cec_func
        self.dim = dim
        self.bounds = cec_func.bounds
        self.lb = self.bounds[:, 0]
        self.ub = self.bounds[:, 1]

    def __call__(self, x):
        """è¯„ä¼°å‡½æ•°å€¼ - é€‚é…åŸºçº¿ç®—æ³•æ¥å£"""
        # åŸºçº¿ç®—æ³•æœŸæœ›æœ€å¤§åŒ–é—®é¢˜ï¼ŒCEC2022æ˜¯æœ€å°åŒ–é—®é¢˜ï¼Œæ‰€ä»¥å–è´Ÿå€¼
        # ç¡®ä¿xåœ¨è¾¹ç•Œå†…
        x = np.clip(x, self.lb, self.ub)
        return -self.cec_func.evaluate(x)  # å–è´Ÿå€¼è½¬æ¢ä¸ºæœ€å¤§åŒ–é—®é¢˜

    def evaluate(self, x):
        """ä¸ºOIOç®—æ³•æä¾›evaluateæ–¹æ³•æ¥å£"""
        # OIOç®—æ³•æœŸæœ›æœ€å°åŒ–é—®é¢˜ï¼Œç›´æ¥è¿”å›åŸå§‹å‡½æ•°å€¼
        x = np.clip(x, self.lb, self.ub)
        return self.cec_func.evaluate(x)

# ============================================================================
# ç®—æ³•åŒ…è£…å™¨åŸºç±»
# ============================================================================

from abc import ABC, abstractmethod

class AlgorithmWrapperBase(ABC):
    """æ‰€æœ‰ç®—æ³•åŒ…è£…å™¨çš„æŠ½è±¡åŸºç±»ï¼Œç»Ÿä¸€æ¥å£"""

    def __init__(self, cost_func, dim, bounds, max_evaluations=20000):
        """
        ç»Ÿä¸€çš„åˆå§‹åŒ–æ¥å£

        Args:
            cost_func: æˆæœ¬å‡½æ•°ï¼ˆInterruptibleCostFuncå®ä¾‹ï¼‰
            dim: é—®é¢˜ç»´åº¦
            bounds: è¾¹ç•Œï¼Œå¯ä»¥æ˜¯tuple (min_bound, max_bound) æˆ– array-like
            max_evaluations: æœ€å¤§è¯„ä¼°æ¬¡æ•°
        """
        self.cost_func = cost_func
        self.dim = dim
        self.max_evaluations = max_evaluations

        # ç»Ÿä¸€è¾¹ç•Œæ ¼å¼å¤„ç†
        if isinstance(bounds, tuple) and len(bounds) == 2:
            # å¦‚æœæ˜¯ (min_val, max_val) æ ¼å¼
            self.bounds = bounds
        elif hasattr(bounds, '__len__') and len(bounds) == 2:
            # å¦‚æœæ˜¯ [min_val, max_val] æ ¼å¼
            self.bounds = (bounds[0], bounds[1])
        else:
            raise ValueError(f"Unsupported bounds format: {bounds}")

    @abstractmethod
    def optimize(self):
        """
        æ‰§è¡Œä¼˜åŒ–

        Returns:
            tuple: (best_position, best_value) æˆ– (None, best_value)
        """
        pass

# ============================================================================
# åŸºçº¿ç®—æ³•é€‚é…å™¨
# ============================================================================

class BaselineAlgorithmAdapter(AlgorithmWrapperBase):
    """åŸºçº¿ç®—æ³•é€‚é…å™¨ï¼Œå°†ç¦»æ•£ä¼˜åŒ–ç®—æ³•é€‚é…åˆ°è¿ç»­ä¼˜åŒ–é—®é¢˜ï¼Œä½¿ç”¨å¼‚å¸¸ä¸­æ–­æ¨¡å¼"""

    def __init__(self, BaselineClass, cost_func, fitness_func_adapter, sequence_length, max_evaluations=20000):
        # è·å–è¾¹ç•Œä¿¡æ¯
        if hasattr(fitness_func_adapter, 'lb') and hasattr(fitness_func_adapter, 'ub'):
            bounds = (fitness_func_adapter.lb[0], fitness_func_adapter.ub[0])  # å‡è®¾æ‰€æœ‰ç»´åº¦è¾¹ç•Œç›¸åŒ
            self.lb = np.array(fitness_func_adapter.lb)
            self.ub = np.array(fitness_func_adapter.ub)
        else:
            # é»˜è®¤è¾¹ç•Œ
            bounds = (-100.0, 100.0)
            self.lb = np.full(sequence_length, -100.0)
            self.ub = np.full(sequence_length, 100.0)

        # è°ƒç”¨çˆ¶ç±»åˆå§‹åŒ–
        super().__init__(cost_func, sequence_length, bounds, max_evaluations)

        # åŸºçº¿ç®—æ³•ç‰¹æœ‰çš„å±æ€§
        self.BaselineClass = BaselineClass
        self.fitness_func_adapter = fitness_func_adapter
        self.sequence_length = sequence_length

    def _adapter_func_max(self, discrete_sequence):
        """å°†ç¦»æ•£åºåˆ—è½¬æ¢ä¸ºè¿ç»­å€¼å¹¶è¯„ä¼°ï¼Œè¿”å›æœ€å¤§åŒ–å€¼"""
        # 1. è½¬æ¢ï¼šå°†[0,1]çš„ç¦»æ•£åºåˆ—æ˜ å°„åˆ°è¿ç»­ç©ºé—´
        continuous_x = self.lb + discrete_sequence * (self.ub - self.lb)
        # 2. è°ƒç”¨å¯ä¸­æ–­çš„å‡½æ•°ï¼ˆè¿”å›æœ€å°åŒ–å€¼ï¼‰
        min_fitness = self.cost_func(continuous_x)
        # 3. è¿”å›æœ€å¤§åŒ–å€¼ï¼ˆåŸºçº¿ç®—æ³•æœŸæœ›æœ€å¤§åŒ–é—®é¢˜ï¼‰
        return -min_fitness

    def optimize(self):
        """ä½¿ç”¨åŸºçº¿ç®—æ³•ä¼˜åŒ–ï¼Œè®©å¼‚å¸¸è‡ªç„¶åœ°å‘ä¸Šå†’æ³¡"""
        # å°† _adapter_func_max ä¼ é€’ç»™åŸºçº¿ç®—æ³•
        algorithm = self.BaselineClass(
            fitness_func=self._adapter_func_max,
            sequence_length=self.sequence_length,
            max_evaluations=self.max_evaluations
        )

        # æ­£å¸¸è¿è¡Œï¼Œè®©å¼‚å¸¸è‡ªç„¶åœ°å‘ä¸Šå†’æ³¡
        _, best_fitness_max = algorithm.optimize()

        # å¦‚æœèƒ½è¿è¡Œåˆ°è¿™é‡Œï¼Œè¯´æ˜æ²¡æœ‰ä¸­æ–­
        true_fitness = -best_fitness_max

        return None, true_fitness

# ============================================================================
# OIOç®—æ³•åŒ…è£…å™¨
# ============================================================================

# ============================================================================
# OIOç®—æ³•åŒ…è£…å™¨
# ============================================================================

class OIOWrapper(AlgorithmWrapperBase):
    """OIOç®—æ³•åŒ…è£…å™¨ï¼Œä½¿ç”¨å¼‚å¸¸ä¸­æ–­æ¨¡å¼"""

    def __init__(self, cost_func, dim, bounds, max_evaluations=20000):
        # è°ƒç”¨çˆ¶ç±»åˆå§‹åŒ–ï¼Œç»Ÿä¸€æ¥å£
        super().__init__(cost_func, dim, bounds, max_evaluations)

    def optimize(self):
        """ä½¿ç”¨OIOç®—æ³•ä¼˜åŒ–ï¼Œè®©å¼‚å¸¸è‡ªç„¶åœ°å‘ä¸Šå†’æ³¡"""
        oio = ChampionOIOAlgorithm(
            cost_func=self.cost_func,
            bounds=self.bounds,
            dim=self.dim
        )

        # è®¡ç®—è¿­ä»£æ¬¡æ•°
        max_iter = max(10, self.max_evaluations // 200)  # ä½¿ç”¨ç»Ÿä¸€çš„æœ€å¤§è¯„ä¼°æ¬¡æ•°

        # æ­£å¸¸è¿è¡Œï¼Œè®©å¼‚å¸¸è‡ªç„¶åœ°å‘ä¸Šå†’æ³¡
        best_pos, best_val = oio.optimize(max_iter=max_iter)

        return best_pos, best_val

# ============================================================================
# åˆå¹¶æ‰€æœ‰ç®—æ³•
# ============================================================================

# åˆå¹¶OIOå’Œæ‰€æœ‰åŸºçº¿ç®—æ³•
ALL_CEC_ALGORITHMS = {
    'OIO': OIOWrapper,
    **ALGORITHMS  # åŒ…å«æ‰€æœ‰15ç§åŸºçº¿ç®—æ³•
}

def test_single_algorithm_cec(alg_name, AlgorithmClass, func_adapter, max_evaluations=20000, num_runs=10, data_manager=None, func_name=None):
    """æµ‹è¯•å•ä¸ªç®—æ³•åœ¨CEC2022å‡½æ•°ä¸Šçš„æ€§èƒ½ - åœ¨å•ä¸ªçº¿ç¨‹ä¸­è¿è¡Œå¤šæ¬¡ï¼Œæ”¯æŒæ—©åœæœºåˆ¶"""
    try:
        fitness_results = []
        time_results = []
        fes_results = []  # æ–°å¢ï¼šFESç»“æœ
        error_results = []  # æ–°å¢ï¼šè¯¯å·®ç»“æœ
        convergence_histories = []
        success_threshold = 1e-8  # CEC2022æˆåŠŸé˜ˆå€¼

        print(f"ğŸ”„ çº¿ç¨‹å¼€å§‹æµ‹è¯• {alg_name} ({num_runs}æ¬¡è¿è¡Œ)...")

        for run in range(num_runs):
            # ä¸ºæ¯æ¬¡è¿è¡Œåˆ›å»ºç‹¬ç«‹çš„å‡½æ•°é€‚é…å™¨å‰¯æœ¬
            run_func_adapter = CEC2022FunctionAdapter(func_adapter.cec_func, func_adapter.dim)

            # åˆ›å»ºå¯ä¸­æ–­çš„æˆæœ¬å‡½æ•°å®ä¾‹
            interruptible_func = InterruptibleCostFunc(
                original_cost_func=run_func_adapter.evaluate,  # OIOå’ŒåŸºçº¿éƒ½ç”¨æœ€å°åŒ–æ¥å£
                max_evaluations=max_evaluations,
                success_threshold=success_threshold
            )

            # åˆ›å»ºç®—æ³•å®ä¾‹ï¼Œå¹¶ä¼ å…¥å¯ä¸­æ–­çš„å‡½æ•°
            if alg_name == 'OIO':
                algorithm = AlgorithmClass(
                    cost_func=interruptible_func,
                    dim=func_adapter.dim,
                    bounds=(run_func_adapter.lb[0], run_func_adapter.ub[0]),
                    max_evaluations=max_evaluations
                )
            else:
                algorithm = BaselineAlgorithmAdapter(
                    BaselineClass=AlgorithmClass,
                    cost_func=interruptible_func,  # åŸºçº¿ç®—æ³•ä¹Ÿä½¿ç”¨
                    fitness_func_adapter=run_func_adapter,  # ä»ç„¶éœ€è¦å®ƒæ¥è·å–è¾¹ç•Œ
                    sequence_length=func_adapter.dim,
                    max_evaluations=max_evaluations
                )

            start_time = time.time()
            best_fitness, fes_consumed = (float('inf'), max_evaluations)  # é»˜è®¤å€¼

            try:
                # è¿è¡Œä¼˜åŒ–
                _, best_fitness = algorithm.optimize()  # ä¸å†éœ€è¦ä»è¿™é‡Œè¿”å›FES
                # å¦‚æœæ­£å¸¸ç»“æŸï¼ŒFESå°±æ˜¯è®¡æ•°å™¨çš„å€¼
                fes_consumed = interruptible_func.evaluation_count
            except TargetPrecisionReached as e:
                # å¦‚æœæˆåŠŸä¸­æ–­
                print(f"  âš¡ï¸ {alg_name} åœ¨ç¬¬ {run+1} æ¬¡è¿è¡Œæå‰ç»ˆæ­¢äº FES={e.fes}")
                best_fitness = e.fitness
                fes_consumed = e.fes
            except RuntimeError as e:
                # å¦‚æœæ˜¯è¯„ä¼°æ¬¡æ•°è€—å°½
                best_fitness = interruptible_func.best_fitness
                fes_consumed = interruptible_func.evaluation_count
            except Exception:
                # æ•è·å…¶ä»–ç®—æ³•å†…éƒ¨é”™è¯¯
                raise

            end_time = time.time()

            run_time = end_time - start_time

            # ç¡®ä¿best_fitnessæ˜¯æ•°å€¼ç±»å‹
            if isinstance(best_fitness, np.ndarray):
                best_fitness = float(best_fitness.item())
            elif not isinstance(best_fitness, (int, float)):
                best_fitness = float(best_fitness)

            # è®¡ç®—è¯¯å·® (CEC2022å‡½æ•°çš„ç†è®ºæœ€ä¼˜å€¼é€šå¸¸æ˜¯0)
            optimal_value = 0.0
            error = best_fitness - optimal_value

            fitness_results.append(best_fitness)
            time_results.append(run_time)
            fes_results.append(fes_consumed)  # æ–°å¢
            error_results.append(error)  # æ–°å¢

            # --- ä¿®æ”¹ï¼šä» interruptible_func è·å–æ”¶æ•›å†å² ---
            convergence_history = interruptible_func.convergence_history.copy()

            if convergence_history:
                print(f"  {alg_name}: æˆåŠŸè·å–æ”¶æ•›å†å²ï¼Œé•¿åº¦ = {len(convergence_history)}")
                convergence_histories.append(convergence_history)

                # ä¿å­˜å•æ¬¡è¿è¡Œçš„æ”¶æ•›æ•°æ®
                if data_manager is not None and func_name is not None:
                    run_id = f"run_{run+1}_{datetime.now().strftime('%H%M%S')}"
                    data_manager.save_convergence_data(func_name, alg_name, convergence_history, run_id)
            else:
                print(f"  {alg_name}: æœªç”Ÿæˆæ”¶æ•›å†å²")
            # --- ä¿®æ”¹ç»“æŸ ---

            # æ¯5æ¬¡è¿è¡ŒæŠ¥å‘Šä¸€æ¬¡è¿›åº¦
            if (run + 1) % 5 == 0:
                print(f"  {alg_name}: å®Œæˆ {run + 1}/{num_runs} æ¬¡è¿è¡Œ")

            # æ¸…ç†
            del algorithm
            del run_func_adapter
            gc.collect()

        # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
        fitness_array = np.array(fitness_results)
        time_array = np.array(time_results)
        fes_array = np.array(fes_results)
        error_array = np.array(error_results)

        # è®¡ç®—æˆåŠŸæ¬¡æ•° (è¯¯å·®å°äºé˜ˆå€¼)
        success_count = np.sum(error_array < success_threshold)

        result = {
            'fitness_mean': np.mean(fitness_array),
            'fitness_std': np.std(fitness_array),
            'fitness_best': np.min(fitness_array),
            'fitness_worst': np.max(fitness_array),
            'time_mean': np.mean(time_array),
            'success_count': success_count,  # ä¿®æ”¹ï¼šå®é™…æˆåŠŸæ¬¡æ•°
            'total_runs': num_runs,
            'all_fitness': fitness_results.copy(),
            'all_times': time_results.copy(),
            'all_fes': fes_results.copy(),  # æ–°å¢ï¼šæ‰€æœ‰FESè®°å½•
            'all_errors': error_results.copy(),  # æ–°å¢ï¼šæ‰€æœ‰è¯¯å·®è®°å½•
            'convergence_histories': convergence_histories.copy() if convergence_histories else []
        }

        print(f"âœ… {alg_name} çº¿ç¨‹å®Œæˆ: æœ€ä¼˜={result['fitness_best']:.6e}, å¹³å‡={result['fitness_mean']:.6e}Â±{result['fitness_std']:.6e}")

        return alg_name, result

    except Exception as e:
        print(f"âŒ {alg_name} çº¿ç¨‹å¤±è´¥: {str(e)}")
        import traceback
        traceback.print_exc()
        return alg_name, {'error': str(e)}

def calculate_cec_style_ranking(func_results, success_threshold=1e-8):
    """
    æ ¹æ®CECç«èµ›è§„åˆ™å¯¹å•ä¸ªå‡½æ•°çš„ç»“æœè¿›è¡Œæ’å

    Args:
        func_results: å•ä¸ªå‡½æ•°çš„æ‰€æœ‰ç®—æ³•ç»“æœ
        success_threshold: æˆåŠŸé˜ˆå€¼

    Returns:
        list: æ’åç»“æœ [(ç®—æ³•å, æ€»åˆ†), ...]ï¼ŒæŒ‰æ€»åˆ†å‡åºæ’åˆ—
    """
    all_runs = []

    # 1. æ±‡é›†æ‰€æœ‰ç®—æ³•çš„æ‰€æœ‰è¿è¡Œç»“æœ
    for alg_name, result_data in func_results.items():
        if 'error' in result_data:
            continue

        if 'all_errors' in result_data and 'all_fes' in result_data:
            errors = result_data['all_errors']
            fes_list = result_data['all_fes']

            for i in range(len(errors)):
                error = errors[i]
                fes = fes_list[i]
                is_success = error < success_threshold

                all_runs.append({
                    "alg_name": alg_name,
                    "success": is_success,
                    "error": error,
                    "fes": fes
                })

    if not all_runs:
        return []

    # 2. æ ¹æ®CECè§„åˆ™æ’åº
    # è§„åˆ™1: æˆåŠŸçš„æ’åœ¨å¤±è´¥çš„å‰é¢ (success=Trueæ’åœ¨å‰é¢)
    # è§„åˆ™2: å¦‚æœéƒ½æˆåŠŸ, FESå°‘çš„æ’åœ¨å‰é¢
    # è§„åˆ™3: å¦‚æœéƒ½å¤±è´¥, errorå°çš„æ’åœ¨å‰é¢
    sorted_runs = sorted(all_runs, key=lambda x: (
        not x['success'],  # æˆåŠŸçš„æ’åœ¨å‰é¢
        x['fes'] if x['success'] else x['error']  # æˆåŠŸçœ‹FESï¼Œå¤±è´¥çœ‹error
    ))

    # 3. åˆ†é…æ’åå¹¶è®¡ç®—æ€»åˆ†
    algorithm_names = set(run['alg_name'] for run in all_runs)
    ranks = {alg_name: 0 for alg_name in algorithm_names}

    for i, run in enumerate(sorted_runs):
        ranks[run['alg_name']] += (i + 1)  # æ’åä»1å¼€å§‹

    # 4. æŒ‰æ€»åˆ†æ’åº (åˆ†æ•°è¶Šä½è¶Šå¥½)
    final_ranking = sorted(ranks.items(), key=lambda x: x[1])
    return final_ranking

def create_statistical_summary_table(statistical_results, output_dir):
    """åˆ›å»ºä¸€ä¸ª Win-Loss-Tie æ€»ç»“è¡¨"""
    if not statistical_results:
        return

    df = pd.DataFrame(statistical_results)
    summary = {}

    for _, row in df.iterrows():
        alg = row['Baseline_Algorithm']
        if alg not in summary:
            summary[alg] = {'Win': 0, 'Loss': 0, 'Tie': 0}

        # OIOçš„p-value < 0.05 æ„å‘³ç€OIOæ˜¾è‘—æ›´å¥½ (Win)
        if row['P_Value'] < 0.05 and row['OIO_Mean'] < row['Baseline_Mean']:
            summary[alg]['Win'] += 1
        # å¦‚æœOIOæ˜¾è‘—æ›´å·®
        elif row['P_Value'] < 0.05 and row['OIO_Mean'] > row['Baseline_Mean']:
            summary[alg]['Loss'] += 1
        else:
            # æ— æ³•æ‹’ç»åŸå‡è®¾ï¼Œè®¤ä¸ºæŒå¹³ (Tie)
            summary[alg]['Tie'] += 1

    summary_df = pd.DataFrame.from_dict(summary, orient='index')
    summary_df.index.name = 'Algorithm vs OIO'
    print("\nğŸ“Š OIO vs åŸºçº¿ç®—æ³•ç»Ÿè®¡æ€»ç»“ (Win-Loss-Tie):")
    print(summary_df.to_string())
    summary_df.to_csv(f'{output_dir}/statistical_summary.csv')

def setup_matplotlib():
    """è®¾ç½®matplotlibå‚æ•°"""
    plt.rcParams['font.family'] = 'DejaVu Sans'
    plt.rcParams['font.size'] = 12
    plt.rcParams['figure.dpi'] = 300
    plt.rcParams['savefig.dpi'] = 300
    plt.rcParams['savefig.bbox'] = 'tight'

def create_performance_boxplot(all_results, cec_functions, output_dir='results', data_manager=None):
    """åˆ›å»ºæ€§èƒ½å¯¹æ¯”ç®±çº¿å›¾"""
    Path(output_dir).mkdir(exist_ok=True)

    # è·å–æ‰€æœ‰å‡ºç°çš„ç®—æ³•åç§°
    all_algorithms = set()
    for func_results in all_results.values():
        all_algorithms.update(func_results.keys())

    # æ”¶é›†æ¯ä¸ªç®—æ³•åœ¨æ‰€æœ‰å‡½æ•°ä¸Šçš„æ€§èƒ½æ•°æ®ï¼ˆç”¨äºç®±çº¿å›¾ï¼‰
    algorithm_all_data = {}  # å­˜å‚¨æ‰€æœ‰åŸå§‹æ•°æ®ç‚¹
    algorithm_performance = {}  # å­˜å‚¨å¹³å‡æ€§èƒ½
    algorithm_std = {}  # å­˜å‚¨æ ‡å‡†å·®

    for alg_name in all_algorithms:
        all_fitness_values = []  # æ”¶é›†æ‰€æœ‰å‡½æ•°çš„æ‰€æœ‰è¿è¡Œç»“æœ
        all_means = []  # æ”¶é›†æ‰€æœ‰å‡½æ•°çš„å¹³å‡å€¼

        for func_name in cec_functions.keys():
            if (alg_name in all_results[func_name] and
                'error' not in all_results[func_name][alg_name]):

                result = all_results[func_name][alg_name]

                # æ”¶é›†è¯¥å‡½æ•°çš„å¹³å‡å€¼
                if 'fitness_mean' in result:
                    all_means.append(result['fitness_mean'])

                # æ”¶é›†è¯¥å‡½æ•°çš„æ‰€æœ‰è¿è¡Œç»“æœ
                if 'all_fitness' in result and result['all_fitness']:
                    all_fitness_values.extend(result['all_fitness'])

        if all_means:
            algorithm_performance[alg_name] = np.mean(all_means)
            algorithm_std[alg_name] = np.std(all_means)
            algorithm_all_data[alg_name] = all_fitness_values

    # ä¿å­˜ç®±çº¿å›¾æ•°æ®
    boxplot_data = {
        'algorithm_performance': algorithm_performance,
        'algorithm_std': algorithm_std,
        'algorithm_all_data': algorithm_all_data,
        'functions': list(cec_functions.keys())
    }

    if data_manager is not None:
        data_manager.save_plot_data('boxplot', boxplot_data)

    # æ‰¾åˆ°æœ€ä¼˜ç®—æ³•ï¼ˆæœ€å°å€¼ï¼‰
    best_alg = min(algorithm_performance.keys(), key=lambda x: algorithm_performance[x])

    # å‡†å¤‡æ•°æ®ï¼ŒOIOæ”¾åœ¨æœ€å·¦è¾¹
    algorithms = ['OIO'] + [alg for alg in sorted(algorithm_performance.keys()) if alg != 'OIO']

    # å‡†å¤‡ç®±çº¿å›¾æ•°æ®
    boxplot_datasets = []
    for alg in algorithms:
        if alg in algorithm_all_data and algorithm_all_data[alg]:
            boxplot_datasets.append(algorithm_all_data[alg])
        else:
            # å¦‚æœæ²¡æœ‰åŸå§‹æ•°æ®ï¼Œä½¿ç”¨å¹³å‡å€¼åˆ›å»ºå•ç‚¹æ•°æ®
            boxplot_datasets.append([algorithm_performance[alg]])

    # åˆ›å»ºç®—æ³•ç®€ç§°æ˜ å°„ï¼ˆç”¨äºæ˜¾ç¤ºï¼‰
    def get_algorithm_display_name(alg_name):
        """è·å–ç®—æ³•çš„æ˜¾ç¤ºåç§°ï¼ˆç®€ç§°ï¼‰"""
        # å¦‚æœç®—æ³•åç§°å·²ç»æ˜¯ç®€ç§°ï¼ˆé•¿åº¦<=4ï¼‰ï¼Œç›´æ¥ä½¿ç”¨
        if len(alg_name) <= 4:
            return alg_name

        # å¸¸è§ç®—æ³•ç®€ç§°æ˜ å°„
        name_mapping = {
            'GeneticAlgorithm': 'GA',
            'HillClimbing': 'HC',
            'SimulatedAnnealing': 'SA',
            'DifferentialEvolution': 'DE',
            'ParticleSwarmOptimization': 'PSO',
            'ParticleSwarm': 'PSO',
            'HarrisHawksOptimization': 'HHO',
            'WhaleOptimization': 'WOA',
            'HybridPSOGWO': 'PSO-GWO',
            'AdaptiveHarrisHawks': 'AHHO',
            'HippopotamusOptimization': 'HO',
            'WalrusOptimization': 'WO',
            'CrestedPorcupineOptimizer': 'CPO',
            'ElkHerdOptimizer': 'EHO',
            'GreylagGooseOptimization': 'GGO',
            'QuokkaSwarmOptimization': 'QSO',
            'AntColony': 'ACO',
            'TabuSearch': 'TS'
        }

        return name_mapping.get(alg_name, alg_name[:4].upper())

    # è·å–æ˜¾ç¤ºç”¨çš„ç®—æ³•åç§°
    display_algorithms = [get_algorithm_display_name(alg) for alg in algorithms]

    # åˆ›å»ºå›¾å½¢
    fig, ax = plt.subplots(figsize=(15, 8))

    # åˆ›å»ºç®±çº¿å›¾
    box_plot = ax.boxplot(boxplot_datasets,
                         patch_artist=True,  # å…è®¸å¡«å……é¢œè‰²
                         showmeans=True,     # æ˜¾ç¤ºå‡å€¼
                         meanline=True,      # å‡å€¼ç”¨çº¿è¡¨ç¤º
                         notch=True,         # æ˜¾ç¤ºç½®ä¿¡åŒºé—´
                         whis=1.5)          # å¼‚å¸¸å€¼æ£€æµ‹èŒƒå›´

    # è®¾ç½®é¢œè‰²ï¼šæœ€ä¼˜ç®—æ³•ç”¨æ·±æ©™è‰²ï¼Œå…¶ä»–ç®—æ³•ç”¨æ·±é’è‰²
    best_color = '#FF6B35'    # æ·±æ©™è‰² - çªå‡ºæ˜¾ç¤ºæœ€ä¼˜ç®—æ³•
    other_color = '#4A90A4'   # æ·±é’è‰² - ä¸“ä¸šä¸”æ˜“è¯»

    # ä¸ºæ¯ä¸ªç®±å­è®¾ç½®é¢œè‰²
    for i, (patch, alg) in enumerate(zip(box_plot['boxes'], algorithms)):
        if alg == best_alg:
            patch.set_facecolor(best_color)
            patch.set_alpha(0.7)
        else:
            patch.set_facecolor(other_color)
            patch.set_alpha(0.7)

    # è®¾ç½®å…¶ä»–å…ƒç´ çš„é¢œè‰²
    for element in ['whiskers', 'fliers', 'medians', 'caps']:
        plt.setp(box_plot[element], color='black', alpha=0.8)

    # è®¾ç½®å‡å€¼çº¿çš„é¢œè‰²
    plt.setp(box_plot['means'], color='red', linewidth=2)

    # è®¾ç½®æ ‡ç­¾å’Œæ ‡é¢˜
    ax.set_xlabel('Algorithms', fontsize=14, fontweight='bold')
    ax.set_ylabel('Fitness Value', fontsize=14, fontweight='bold')
    ax.set_title('Performance Comparison Across All CEC2022 Functions (Box Plot)', fontsize=16, fontweight='bold')

    # è®¾ç½®xè½´æ ‡ç­¾
    ax.set_xticks(range(1, len(algorithms) + 1))
    ax.set_xticklabels(display_algorithms, rotation=45, ha='right')

    # æ·»åŠ ç½‘æ ¼
    ax.grid(True, alpha=0.3, axis='y')

    # ä½¿ç”¨å¯¹æ•°åˆ»åº¦ï¼ˆå¦‚æœæ•°æ®èŒƒå›´å¾ˆå¤§ï¼‰
    ax.set_yscale('log')

    # æ·»åŠ å›¾ä¾‹
    from matplotlib.patches import Patch
    from matplotlib.lines import Line2D
    legend_elements = [
        Patch(facecolor=best_color, alpha=0.7, label='Best Algorithm'),
        Patch(facecolor=other_color, alpha=0.7, label='Other Algorithms'),
        Line2D([0], [0], color='red', linewidth=2, label='Mean Value')
    ]
    ax.legend(handles=legend_elements, loc='upper right')

    plt.tight_layout()
    plt.savefig(f'{output_dir}/performance_comparison_boxplot.png', dpi=300, bbox_inches='tight')
    plt.close()

    print(f"ğŸ“Š æ€§èƒ½å¯¹æ¯”ç®±çº¿å›¾å·²ä¿å­˜: {output_dir}/performance_comparison_boxplot.png")

def create_convergence_plots(all_results, cec_functions, output_dir='results'):
    """åˆ›å»ºæ”¶æ•›æ›²çº¿å›¾ï¼Œæ¨ªåæ ‡ä¸ºFESï¼Œä¸å¡«å……æ—©åœæ›²çº¿"""
    convergence_dir = Path(output_dir) / 'convergence_curves'
    convergence_dir.mkdir(parents=True, exist_ok=True)

    for func_name in cec_functions.keys():
        if func_name not in all_results:
            continue

        # è·å–è¯¥å‡½æ•°çš„æ‰€æœ‰ç®—æ³•
        func_algorithms = list(all_results[func_name].keys())
        if not func_algorithms:
            continue

        fig, ax = plt.subplots(figsize=(12, 8))

        # ä¸ºæ¯ä¸ªç®—æ³•ç»˜åˆ¶æ”¶æ•›æ›²çº¿
        import matplotlib.cm as cm

        # ä¸º16ç§ç®—æ³•å‡†å¤‡è¶³å¤Ÿçš„é¢œè‰²
        # ä½¿ç”¨å¤šä¸ªé¢œè‰²æ˜ å°„ç»„åˆï¼Œç¡®ä¿æœ‰è¶³å¤Ÿçš„åŒºåˆ†åº¦
        def get_algorithm_colors(n_algorithms):
            """ä¸ºnä¸ªç®—æ³•ç”Ÿæˆè¶³å¤ŸåŒºåˆ†çš„é¢œè‰²"""
            if n_algorithms <= 20:
                # ä½¿ç”¨tab20é¢œè‰²æ˜ å°„ï¼Œæœ€å¤šæ”¯æŒ20ç§é¢œè‰²
                return cm.get_cmap('tab20')(np.linspace(0, 1, n_algorithms))
            else:
                # å¦‚æœè¶…è¿‡20ç§ï¼Œç»„åˆå¤šä¸ªé¢œè‰²æ˜ å°„
                colors1 = cm.get_cmap('tab20')(np.linspace(0, 1, 20))
                colors2 = cm.get_cmap('Set3')(np.linspace(0, 1, n_algorithms - 20))
                return np.vstack([colors1, colors2])

        colors = get_algorithm_colors(len(func_algorithms))

        # åˆ›å»ºç®—æ³•ç®€ç§°æ˜ å°„å‡½æ•°ï¼ˆä¸ç›´æ–¹å›¾ä¸­ç›¸åŒï¼‰
        def get_algorithm_display_name(alg_name):
            """è·å–ç®—æ³•çš„æ˜¾ç¤ºåç§°ï¼ˆç®€ç§°ï¼‰"""
            if len(alg_name) <= 4:
                return alg_name

            name_mapping = {
                'GeneticAlgorithm': 'GA',
                'HillClimbing': 'HC',
                'SimulatedAnnealing': 'SA',
                'DifferentialEvolution': 'DE',
                'ParticleSwarmOptimization': 'PSO',
                'ParticleSwarm': 'PSO',
                'HarrisHawksOptimization': 'HHO',
                'WhaleOptimization': 'WOA',
                'HybridPSOGWO': 'PSO-GWO',
                'AdaptiveHarrisHawks': 'AHHO',
                'HippopotamusOptimization': 'HO',
                'WalrusOptimization': 'WO',
                'CrestedPorcupineOptimizer': 'CPO',
                'ElkHerdOptimizer': 'EHO',
                'GreylagGooseOptimization': 'GGO',
                'QuokkaSwarmOptimization': 'QSO',
                'AntColony': 'ACO',
                'TabuSearch': 'TS'
            }

            return name_mapping.get(alg_name, alg_name[:4].upper())

        # ä¸ºç®—æ³•åˆ†é…é¢œè‰²ç´¢å¼•ï¼Œç¡®ä¿OIOå§‹ç»ˆä½¿ç”¨å›ºå®šé¢œè‰²
        color_indices = {}
        oio_color = '#FF6B35'  # OIOä¸“ç”¨é¢œè‰²
        other_color_idx = 0

        for alg_name in func_algorithms:
            if alg_name == 'OIO':
                color_indices[alg_name] = 'OIO'  # ç‰¹æ®Šæ ‡è®°
            else:
                color_indices[alg_name] = other_color_idx
                other_color_idx += 1

        # é‡æ–°æ’åºç®—æ³•åˆ—è¡¨ï¼Œç¡®ä¿OIOæœ€åç»˜åˆ¶ï¼ˆæ˜¾ç¤ºåœ¨æœ€ä¸Šå±‚ï¼‰
        other_algorithms = [alg for alg in func_algorithms if alg != 'OIO']
        ordered_algorithms = other_algorithms + (['OIO'] if 'OIO' in func_algorithms else [])

        for alg_name in ordered_algorithms:
            if (alg_name in all_results[func_name] and
                'error' not in all_results[func_name][alg_name] and
                all_results[func_name][alg_name].get('convergence_histories')):

                # histories æ˜¯ä¸€ä¸ªåˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ æ˜¯ä¸€æ¬¡è¿è¡Œçš„æ”¶æ•›å†å²
                # ä¾‹å¦‚: [[(fes, fit), (fes, fit), ...], [(fes, fit), ...]]
                histories = all_results[func_name][alg_name]['convergence_histories']

                if histories and histories[0]: # ç¡®ä¿å†å²è®°å½•ä¸ä¸ºç©º
                    # --- æ ¸å¿ƒä¿®æ”¹å¼€å§‹ ---

                    # 1. æå–æ‰€æœ‰è¿è¡Œçš„FESå’ŒFitnessæ•°æ®
                    all_fes_data = [np.array([point[0] for point in run]) for run in histories]
                    all_fit_data = [np.array([point[1] for point in run]) for run in histories]

                    # 2. åˆ›å»ºä¸€ä¸ªç»Ÿä¸€çš„FESæ’å€¼è½´
                    #    æ‰¾åˆ°æ‰€æœ‰è¿è¡Œä¸­FESçš„æœ€å¤§å€¼ï¼Œå¹¶åˆ›å»ºä¸€ä¸ªå…¬å…±çš„FESè½´
                    max_fes_overall = max(fes_run[-1] for fes_run in all_fes_data if len(fes_run) > 0)
                    # æˆ‘ä»¬åˆ›å»º500ä¸ªæ’å€¼ç‚¹
                    common_fes_axis = np.linspace(1, max_fes_overall, 500)

                    # 3. å¯¹æ¯æ¬¡è¿è¡Œçš„æ”¶æ•›æ›²çº¿è¿›è¡Œæ’å€¼
                    #    è¿™æ ·ï¼Œæ‰€æœ‰æ›²çº¿éƒ½åœ¨ç›¸åŒçš„FESç‚¹ä¸Šæœ‰äº†å¯¹åº”çš„fitnesså€¼
                    interpolated_fits = []
                    for fes_run, fit_run in zip(all_fes_data, all_fit_data):
                        if len(fes_run) > 1:
                            # np.interp è¦æ±‚xåæ ‡å•è°ƒé€’å¢
                            interp_fit = np.interp(common_fes_axis, fes_run, fit_run)
                            interpolated_fits.append(interp_fit)

                    if not interpolated_fits: continue # å¦‚æœæ²¡æœ‰å¯æ’å€¼çš„æ•°æ®ï¼Œåˆ™è·³è¿‡

                    # 4. è®¡ç®—æ’å€¼åæ›²çº¿çš„å¹³å‡å€¼å’Œæ ‡å‡†å·®
                    mean_history = np.mean(interpolated_fits, axis=0)
                    std_history = np.std(interpolated_fits, axis=0)

                    # 5. æ‰¾åˆ°è¯¥ç®—æ³•å®é™…åœæ­¢çš„å¹³å‡FES
                    #    æˆ‘ä»¬åªå°†æ›²çº¿ç”»åˆ°è¿™ä¸ªç‚¹ä¸ºæ­¢
                    avg_stop_fes = np.mean([fes_run[-1] for fes_run in all_fes_data if len(fes_run) > 0])

                    # 6. æˆªæ–­å…¬å…±FESè½´å’Œå¹³å‡æ›²çº¿
                    plot_mask = common_fes_axis <= avg_stop_fes
                    plot_fes_axis = common_fes_axis[plot_mask]
                    plot_mean_history = mean_history[plot_mask]
                    plot_std_history = std_history[plot_mask]

                    # --- æ ¸å¿ƒä¿®æ”¹ç»“æŸ ---

                    # åˆ†é…é¢œè‰²ï¼šOIOç”¨å›ºå®šé¢œè‰²ï¼Œå…¶ä»–ç®—æ³•ç”¨ä¸åŒé¢œè‰²
                    if alg_name == 'OIO':
                        color = oio_color
                        linewidth = 3
                        alpha = 0.9
                    else:
                        color_idx = color_indices[alg_name]
                        color = colors[color_idx] if color_idx < len(colors) else colors[color_idx % len(colors)]
                        linewidth = 2
                        alpha = 0.7

                    # ä½¿ç”¨ç®€ç§°ä½œä¸ºå›¾ä¾‹æ ‡ç­¾
                    display_name = get_algorithm_display_name(alg_name)

                    # ä½¿ç”¨æ–°çš„FESè½´å’Œæˆªæ–­åçš„æ•°æ®è¿›è¡Œç»˜å›¾
                    ax.plot(plot_fes_axis, plot_mean_history, color=color, linewidth=linewidth,
                           label=display_name, alpha=alpha)

                    # æ·»åŠ æ ‡å‡†å·®é˜´å½±
                    ax.fill_between(plot_fes_axis,
                                   plot_mean_history - plot_std_history,
                                   plot_mean_history + plot_std_history,
                                   color=color, alpha=0.15)

        # --- ä¿®æ”¹åæ ‡è½´æ ‡ç­¾ ---
        ax.set_xlabel('Function Evaluations (FES)', fontsize=14, fontweight='bold')
        ax.set_ylabel('Best Fitness Value', fontsize=14, fontweight='bold')
        ax.set_title(f'Convergence Curves for {func_name}', fontsize=16, fontweight='bold')
        ax.grid(True, alpha=0.3)

        # ä¼˜åŒ–å›¾ä¾‹æ˜¾ç¤ºï¼Œé€‚åº”16ç§ç®—æ³•
        n_algorithms = len([alg for alg in ordered_algorithms
                           if (alg in all_results[func_name] and
                               'error' not in all_results[func_name][alg] and
                               all_results[func_name][alg].get('convergence_histories'))])

        # è·å–å›¾ä¾‹å¥æŸ„å’Œæ ‡ç­¾ï¼Œé‡æ–°æ’åºä½¿OIOæ˜¾ç¤ºåœ¨æœ€å‰é¢
        handles, labels = ax.get_legend_handles_labels()

        # é‡æ–°æ’åºï¼šOIOåœ¨å‰ï¼Œå…¶ä»–ç®—æ³•æŒ‰å­—æ¯é¡ºåº
        oio_items = [(h, l) for h, l in zip(handles, labels) if l == 'OIO']
        other_items = [(h, l) for h, l in zip(handles, labels) if l != 'OIO']
        other_items.sort(key=lambda x: x[1])  # æŒ‰æ ‡ç­¾æ’åº

        # åˆå¹¶ï¼šOIOåœ¨å‰
        ordered_items = oio_items + other_items
        ordered_handles = [item[0] for item in ordered_items]
        ordered_labels = [item[1] for item in ordered_items]

        if n_algorithms <= 8:
            # ç®—æ³•è¾ƒå°‘æ—¶ï¼Œå›¾ä¾‹æ”¾åœ¨å³ä¾§
            ax.legend(ordered_handles, ordered_labels,
                     bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=10)
        else:
            # ç®—æ³•è¾ƒå¤šæ—¶ï¼Œå›¾ä¾‹æ”¾åœ¨ä¸‹æ–¹ï¼Œåˆ†å¤šåˆ—æ˜¾ç¤º
            ncol = min(4, (n_algorithms + 3) // 4)  # æœ€å¤š4åˆ—
            ax.legend(ordered_handles, ordered_labels,
                     bbox_to_anchor=(0.5, -0.15), loc='upper center',
                     ncol=ncol, fontsize=9)

        ax.set_yscale('log')  # ä½¿ç”¨å¯¹æ•°åˆ»åº¦æ›´å¥½åœ°æ˜¾ç¤ºæ”¶æ•›è¿‡ç¨‹

        plt.tight_layout()
        plt.savefig(f'{convergence_dir}/{func_name}_convergence.png', dpi=300, bbox_inches='tight')
        plt.close()

    print(f"ğŸ“ˆ æ”¶æ•›æ›²çº¿å›¾å·²ä¿å­˜åˆ°: {convergence_dir}/ (æ¨ªåæ ‡ä¸ºFES)")

def perform_statistical_tests(all_results, cec_functions, output_dir='results'):
    """æ‰§è¡Œç»Ÿè®¡æ£€éªŒ"""
    Path(output_dir).mkdir(exist_ok=True)

    statistical_results = []

    print("\nğŸ”¬ æ‰§è¡Œç»Ÿè®¡æ£€éªŒ (Mann-Whitney Uæ£€éªŒ)...")

    for func_name in cec_functions.keys():
        if 'OIO' not in all_results[func_name] or 'error' in all_results[func_name]['OIO']:
            continue

        oio_data = all_results[func_name]['OIO']
        if 'all_fitness' not in oio_data:
            continue

        oio_results = oio_data['all_fitness']

        # è·å–è¯¥å‡½æ•°çš„æ‰€æœ‰ç®—æ³•
        for alg_name in all_results[func_name].keys():
            if alg_name == 'OIO':
                continue

            if ('error' not in all_results[func_name][alg_name] and
                'all_fitness' in all_results[func_name][alg_name]):

                baseline_results = all_results[func_name][alg_name]['all_fitness']

                # æ‰§è¡ŒMann-Whitney Uæ£€éªŒï¼ˆé€‚ç”¨äºç‹¬ç«‹æ ·æœ¬ï¼‰
                try:
                    statistic, p_value = mannwhitneyu(oio_results, baseline_results,
                                                     alternative='less')  # OIOæ˜¯å¦æ˜¾è‘—æ›´å°

                    # è®¡ç®—æ•ˆåº”å¤§å° (Cohen's d)
                    pooled_std = np.sqrt((np.var(oio_results) + np.var(baseline_results)) / 2)
                    cohens_d = (np.mean(oio_results) - np.mean(baseline_results)) / pooled_std

                    significance = 'Yes' if p_value < 0.05 else 'No'

                    statistical_results.append({
                        'Function': func_name,
                        'Baseline_Algorithm': alg_name,
                        'OIO_Mean': np.mean(oio_results),
                        'Baseline_Mean': np.mean(baseline_results),
                        'U_Statistic': statistic,
                        'P_Value': p_value,
                        'Significant': significance,
                        'Cohens_D': cohens_d,
                        'Effect_Size': 'Large' if abs(cohens_d) > 0.8 else 'Medium' if abs(cohens_d) > 0.5 else 'Small'
                    })

                except Exception as e:
                    print(f"  âš ï¸ {func_name} vs {alg_name}: ç»Ÿè®¡æ£€éªŒå¤±è´¥ - {e}")

    # ä¿å­˜ç»Ÿè®¡æ£€éªŒç»“æœ
    if statistical_results:
        stats_df = pd.DataFrame(statistical_results)
        stats_file = f'{output_dir}/statistical_tests.csv'
        stats_df.to_csv(stats_file, index=False, encoding='utf-8-sig')
        print(f"ğŸ“Š ç»Ÿè®¡æ£€éªŒç»“æœå·²ä¿å­˜: {stats_file}")

        # æ˜¾ç¤ºæ˜¾è‘—æ€§ç»“æœæ‘˜è¦
        significant_count = len(stats_df[stats_df['Significant'] == 'Yes'])
        total_count = len(stats_df)
        print(f"ğŸ“ˆ æ˜¾è‘—æ€§ç»“æœ: {significant_count}/{total_count} ä¸ªæ¯”è¾ƒä¸­OIOæ˜¾è‘—ä¼˜äºåŸºçº¿ç®—æ³•")

    return statistical_results

def main(load_existing_data=False, run_id=None):
    """ä¸»å‡½æ•°

    Args:
        load_existing_data: æ˜¯å¦åŠ è½½å·²æœ‰æ•°æ®è€Œä¸é‡æ–°è¿è¡Œå®éªŒ
        run_id: æŒ‡å®šè¦åŠ è½½çš„å®éªŒIDï¼Œå¦‚æœä¸ºNoneåˆ™åŠ è½½æœ€æ–°çš„
    """
    # è®¾ç½®matplotlib
    setup_matplotlib()

    # åˆå§‹åŒ–æ•°æ®ç®¡ç†å™¨
    data_manager = DataManager()

    # å¯åŠ¨å†…å­˜ç›‘æ§
    tracemalloc.start()
    initial_memory = get_memory_usage()

    print("ğŸŒŠ CEC2022åŸºå‡†æµ‹è¯• - OIO vs åŸºçº¿ç®—æ³•")
    print("=" * 80)
    print(f"åˆå§‹å†…å­˜ä½¿ç”¨: {initial_memory:.1f} MB")

    # æµ‹è¯•é…ç½®
    dim = 10  # CEC2022æ ‡å‡†ç»´åº¦
    num_runs = 10  # æ¯ä¸ªç®—æ³•è¿è¡Œæ¬¡æ•°
    max_evaluations = 20000  # æœ€å¤§è¯„ä¼°æ¬¡æ•°

    # CEC2022å‡½æ•°åˆ—è¡¨
    cec_functions = {
        'F1': F12022(ndim=dim),
        'F2': F22022(ndim=dim),
        'F3': F32022(ndim=dim),
        'F4': F42022(ndim=dim),
        'F5': F52022(ndim=dim),
        'F6': F62022(ndim=dim),
        'F7': F72022(ndim=dim),
        'F8': F82022(ndim=dim),
        'F9': F92022(ndim=dim),
        'F10': F102022(ndim=dim),
        'F11': F112022(ndim=dim),
        'F12': F122022(ndim=dim)
    }

    experiment_config = {
        'dim': dim,
        'num_runs': num_runs,
        'max_evaluations': max_evaluations,
        'functions': list(cec_functions.keys()),
        'algorithms': list(ALL_CEC_ALGORITHMS.keys())
    }

    print(f"æµ‹è¯•é…ç½®:")
    print(f"  ç»´åº¦: {dim}D")
    print(f"  å‡½æ•°æ•°é‡: {len(cec_functions)}")
    print(f"  ç®—æ³•æ•°é‡: {len(ALL_CEC_ALGORITHMS)}")
    print(f"  æ¯ä¸ªç®—æ³•è¿è¡Œæ¬¡æ•°: {num_runs}")
    print(f"  æœ€å¤§è¯„ä¼°æ¬¡æ•°: {max_evaluations}")
    print(f"  æ€»æµ‹è¯•æ¬¡æ•°: {len(cec_functions) * len(ALL_CEC_ALGORITHMS) * num_runs}")

    # æ£€æŸ¥æ˜¯å¦åŠ è½½å·²æœ‰æ•°æ®
    if load_existing_data:
        print(f"\nğŸ“‚ å°è¯•åŠ è½½å·²æœ‰å®éªŒæ•°æ®...")
        data_manager.list_available_data()

        all_results, loaded_config = data_manager.load_experiment_results(run_id)
        if all_results is not None:
            print(f"âœ… æˆåŠŸåŠ è½½å®éªŒæ•°æ®!")
            if loaded_config:
                print(f"   åŠ è½½çš„é…ç½®: {loaded_config}")

            # ä½¿ç”¨åŠ è½½çš„æ•°æ®è¿›è¡Œå¯è§†åŒ–å’Œåˆ†æ
            output_dir = 'results'
            Path(output_dir).mkdir(exist_ok=True)

            print(f"\nğŸ¨ ä½¿ç”¨åŠ è½½çš„æ•°æ®ç”Ÿæˆå¯è§†åŒ–åˆ†æ...")
            try:
                create_performance_boxplot(all_results, cec_functions, output_dir, data_manager)
                create_convergence_plots(all_results, cec_functions, output_dir)
                perform_statistical_tests(all_results, cec_functions, output_dir)
                print(f"âœ… å¯è§†åŒ–åˆ†æå®Œæˆ!")
            except Exception as e:
                print(f"âš ï¸ å¯è§†åŒ–ç”Ÿæˆå¤±è´¥: {e}")

            return
        else:
            print(f"âŒ æœªæ‰¾åˆ°å¯ç”¨çš„å®éªŒæ•°æ®ï¼Œå°†é‡æ–°è¿è¡Œå®éªŒ...")

    # å­˜å‚¨æ‰€æœ‰ç»“æœ
    all_results = {}

    # å¯¹æ¯ä¸ªå‡½æ•°è¿›è¡Œæµ‹è¯•
    for func_idx, (func_name, cec_func) in enumerate(cec_functions.items(), 1):
        print(f"\n{'='*60}")
        print(f"å‡½æ•° {func_idx}/{len(cec_functions)}: {func_name} - {cec_func.__class__.__name__}")
        print(f"{'='*60}")

        # åˆ›å»ºå‡½æ•°é€‚é…å™¨
        func_adapter = CEC2022FunctionAdapter(cec_func, dim)
        print(f"æœç´¢èŒƒå›´: [{func_adapter.lb[0]:.1f}, {func_adapter.ub[0]:.1f}]")

        # ä½¿ç”¨16çº¿ç¨‹å¹¶è¡Œæµ‹è¯•æ‰€æœ‰ç®—æ³•
        func_results = {}
        with ThreadPoolExecutor(max_workers=16) as executor:
            # æäº¤æ‰€æœ‰ä»»åŠ¡
            future_to_alg = {
                executor.submit(test_single_algorithm_cec, alg_name, AlgorithmClass, func_adapter, max_evaluations, num_runs, data_manager, func_name): alg_name
                for alg_name, AlgorithmClass in ALL_CEC_ALGORITHMS.items()
            }

            # æ”¶é›†ç»“æœ
            completed = 0
            for future in as_completed(future_to_alg):
                alg_name = future_to_alg[future]
                completed += 1

                try:
                    result_name, result_data = future.result()
                    func_results[result_name] = result_data

                    if 'error' not in result_data:
                        fitness_mean = result_data['fitness_mean']
                        fitness_std = result_data['fitness_std']
                        time_mean = result_data['time_mean']
                        print(f"[{completed:2d}/{len(ALL_CEC_ALGORITHMS)}] âœ… {alg_name:<8} å®Œæˆ: {fitness_mean:.6e}Â±{fitness_std:.4e} ({time_mean:.2f}s)")
                    else:
                        print(f"[{completed:2d}/{len(ALL_CEC_ALGORITHMS)}] âŒ {alg_name:<8} é”™è¯¯: {result_data['error']}")

                except Exception as e:
                    print(f"[{completed:2d}/{len(ALL_CEC_ALGORITHMS)}] âŒ {alg_name:<8} å¼‚å¸¸: {e}")
                    func_results[alg_name] = {'error': str(e)}

        all_results[func_name] = func_results

        # æ˜¾ç¤ºè¯¥å‡½æ•°çš„CECé£æ ¼æ’å
        cec_ranking = calculate_cec_style_ranking(func_results)
        if cec_ranking:
            print(f"\nğŸ† {func_name} CECé£æ ¼æ’å (åŸºäºæ‰€æœ‰è¿è¡Œå®ä¾‹):")
            for rank, (alg_name, total_score) in enumerate(cec_ranking, 1):
                star = 'â­' if alg_name == 'OIO' else '  '
                print(f"  {rank}. {star} {alg_name}: æ€»åˆ† {total_score}")

        # åŒæ—¶æ˜¾ç¤ºä¼ ç»Ÿçš„å¹³å‡å€¼æ’åä½œä¸ºå¯¹æ¯”
        valid_results = {k: v for k, v in func_results.items() if 'error' not in v}
        if valid_results:
            sorted_results = sorted(valid_results.items(), key=lambda x: x[1]['fitness_mean'])
            print(f"\nğŸ“Š {func_name} ä¼ ç»Ÿæ’å (æŒ‰å¹³å‡å€¼):")
            for rank, (alg_name, result) in enumerate(sorted_results, 1):
                star = 'â­' if alg_name == 'OIO' else '  '
                print(f"  {rank}. {star} {alg_name}: {result['fitness_mean']:.6e}")

    # ç”Ÿæˆç»¼åˆç»“æœè¡¨æ ¼
    print(f"\nğŸ† CEC2022ç»¼åˆåŸºå‡†æµ‹è¯•ç»“æœ")
    print("=" * 100)

    # å‡†å¤‡è¡¨æ ¼æ•°æ®
    table_data = []
    for func_name in sorted(cec_functions.keys()):
        for alg_name in sorted(ALL_CEC_ALGORITHMS.keys()):
            result = all_results[func_name][alg_name]
            if 'error' not in result:
                table_data.append({
                    'å‡½æ•°': func_name,
                    'ç®—æ³•': alg_name,
                    'æœ€ä¼˜å€¼': f"{result['fitness_best']:.6e}",
                    'å¹³å‡å€¼': f"{result['fitness_mean']:.6e}",
                    'æ ‡å‡†å·®': f"{result['fitness_std']:.6e}",
                    'å¹³å‡æ—¶é—´(s)': f"{result['time_mean']:.2f}"
                })
            else:
                table_data.append({
                    'å‡½æ•°': func_name,
                    'ç®—æ³•': alg_name,
                    'æœ€ä¼˜å€¼': 'ERROR',
                    'å¹³å‡å€¼': 'ERROR',
                    'æ ‡å‡†å·®': 'ERROR',
                    'å¹³å‡æ—¶é—´(s)': 'ERROR'
                })

    # åˆ›å»ºDataFrameå¹¶æ‰“å°
    df = pd.DataFrame(table_data)
    print(df.to_string(index=False))

    # åˆ›å»ºç»“æœç›®å½•
    output_dir = 'results'
    Path(output_dir).mkdir(exist_ok=True)

    # ä¿å­˜è¯¦ç»†ç»“æœ
    detailed_file = f'{output_dir}/cec2022_detailed_results.csv'
    df.to_csv(detailed_file, index=False, encoding='utf-8-sig')
    print(f"\nğŸ’¾ è¯¦ç»†ç»“æœå·²ä¿å­˜åˆ°: {detailed_file}")

    # è®¡ç®—æ¯ä¸ªç®—æ³•çš„æ€»ä½“æ’å - CECé£æ ¼
    print(f"\nğŸ¥‡ ç®—æ³•æ€»ä½“æ’ååˆ†æ:")

    # CECé£æ ¼æ€»æ’åï¼šå°†æ¯ä¸ªå‡½æ•°çš„CECæ’ååˆ†æ•°ç›¸åŠ 
    cec_total_scores = {}
    traditional_ranks = {}

    for func_name in cec_functions.keys():
        # CECé£æ ¼æ’å
        cec_ranking = calculate_cec_style_ranking(all_results[func_name])
        for alg_name, score in cec_ranking:
            if alg_name not in cec_total_scores:
                cec_total_scores[alg_name] = 0
            cec_total_scores[alg_name] += score

        # ä¼ ç»Ÿå¹³å‡å€¼æ’å
        valid_results = {k: v for k, v in all_results[func_name].items() if 'error' not in v}
        if valid_results:
            sorted_results = sorted(valid_results.items(), key=lambda x: x[1]['fitness_mean'])
            for rank, (alg_name, _) in enumerate(sorted_results, 1):
                if alg_name not in traditional_ranks:
                    traditional_ranks[alg_name] = []
                traditional_ranks[alg_name].append(rank)

    # CECé£æ ¼æœ€ç»ˆæ’å (æ€»åˆ†è¶Šä½è¶Šå¥½)
    sorted_cec_ranks = sorted(cec_total_scores.items(), key=lambda x: x[1])

    print("ğŸ† CECé£æ ¼æ€»æ’å (åŸºäºæ‰€æœ‰è¿è¡Œå®ä¾‹çš„ç»¼åˆæ’ååˆ†æ•°):")
    for rank, (alg_name, total_score) in enumerate(sorted_cec_ranks, 1):
        star = 'â­' if alg_name == 'OIO' else '  '
        print(f"  {rank}. {star} {alg_name}: æ€»åˆ† {total_score}")

    # ä¼ ç»Ÿå¹³å‡æ’å
    avg_ranks = {}
    for alg_name, ranks in traditional_ranks.items():
        avg_ranks[alg_name] = np.mean(ranks)

    sorted_avg_ranks = sorted(avg_ranks.items(), key=lambda x: x[1])

    print("\nğŸ“Š ä¼ ç»Ÿæ€»æ’å (æŒ‰å¹³å‡æ’å):")
    for rank, (alg_name, avg_rank) in enumerate(sorted_avg_ranks, 1):
        star = 'â­' if alg_name == 'OIO' else '  '
        print(f"  {rank}. {star} {alg_name}: å¹³å‡æ’å {avg_rank:.2f}")

    # ä¿å­˜æ’åç»“æœ - åŒ…å«CECé£æ ¼å’Œä¼ ç»Ÿæ’å
    ranking_data = []

    # åˆ›å»ºç®—æ³•åˆ°æ’åçš„æ˜ å°„
    cec_rank_map = {alg: rank for rank, (alg, _) in enumerate(sorted_cec_ranks, 1)}
    traditional_rank_map = {alg: rank for rank, (alg, _) in enumerate(sorted_avg_ranks, 1)}

    all_algorithms = set(cec_total_scores.keys()) | set(avg_ranks.keys())

    for alg_name in all_algorithms:
        ranking_data.append({
            'Algorithm': alg_name,
            'CEC_Rank': cec_rank_map.get(alg_name, 'N/A'),
            'CEC_Total_Score': cec_total_scores.get(alg_name, 'N/A'),
            'Traditional_Rank': traditional_rank_map.get(alg_name, 'N/A'),
            'Traditional_Avg_Rank': avg_ranks.get(alg_name, 'N/A'),
            'Is_OIO': 'Yes' if alg_name == 'OIO' else 'No'
        })

    ranking_df = pd.DataFrame(ranking_data)
    # æŒ‰CECæ’åæ’åº
    ranking_df = ranking_df.sort_values('CEC_Rank')

    ranking_file = f'{output_dir}/algorithm_rankings.csv'
    ranking_df.to_csv(ranking_file, index=False, encoding='utf-8-sig')
    print(f"ğŸ“Š æ’åç»“æœå·²ä¿å­˜åˆ°: {ranking_file}")

    # ä¿å­˜å®Œæ•´çš„å®éªŒæ•°æ®
    print(f"\nğŸ’¾ ä¿å­˜å®éªŒæ•°æ®...")
    current_run_id = datetime.now().strftime("%Y%m%d_%H%M%S")
    data_manager.save_experiment_results(all_results, experiment_config, current_run_id)
    print(f"âœ… å®éªŒæ•°æ®å·²ä¿å­˜ (ID: {current_run_id})")

    # æ‰§è¡Œå¯è§†åŒ–åˆ†æ
    print(f"\nğŸ¨ ç”Ÿæˆå¯è§†åŒ–åˆ†æ...")
    try:
        create_performance_boxplot(all_results, cec_functions, output_dir, data_manager)
        create_convergence_plots(all_results, cec_functions, output_dir)
    except Exception as e:
        print(f"âš ï¸ å¯è§†åŒ–ç”Ÿæˆå¤±è´¥: {e}")

    # æ‰§è¡Œç»Ÿè®¡æ£€éªŒ
    try:
        statistical_results = perform_statistical_tests(all_results, cec_functions, output_dir)
        # åˆ›å»ºç»Ÿè®¡æ€»ç»“è¡¨
        if statistical_results:
            create_statistical_summary_table(statistical_results, output_dir)
    except Exception as e:
        print(f"âš ï¸ ç»Ÿè®¡æ£€éªŒå¤±è´¥: {e}")

    # å†…å­˜æ¸…ç†
    final_memory = get_memory_usage()
    memory_increase = final_memory - initial_memory

    print(f"\nğŸ§¹ å†…å­˜ç®¡ç†æŠ¥å‘Š:")
    print(f"åˆå§‹å†…å­˜: {initial_memory:.1f} MB")
    print(f"æœ€ç»ˆå†…å­˜: {final_memory:.1f} MB")
    print(f"å†…å­˜å¢é•¿: {memory_increase:.1f} MB")

    print(f"\nğŸ“ æ‰€æœ‰ç»“æœå·²ä¿å­˜åˆ°:")
    print(f"  ğŸ“Š CSVç»“æœæ–‡ä»¶ ('{output_dir}' ç›®å½•):")
    print(f"    - {detailed_file}: è¯¦ç»†å®éªŒç»“æœ")
    print(f"    - {ranking_file}: ç®—æ³•æ’å")
    print(f"    - statistical_tests.csv: ç»Ÿè®¡æ£€éªŒç»“æœ")
    print(f"  ğŸ“ˆ å¯è§†åŒ–æ–‡ä»¶ ('{output_dir}' ç›®å½•):")
    print(f"    - performance_comparison_boxplot.png: æ€§èƒ½å¯¹æ¯”ç®±çº¿å›¾")
    print(f"    - convergence_curves/: æ”¶æ•›æ›²çº¿å›¾")
    print(f"  ğŸ’¾ å®éªŒæ•°æ® ('{data_manager.data_dir}' ç›®å½•):")
    print(f"    - results_data/: å®Œæ•´å®éªŒç»“æœ (å¯é‡æ–°åŠ è½½)")
    print(f"    - convergence_data/: æ”¶æ•›æ›²çº¿åŸå§‹æ•°æ®")
    print(f"    - plot_data/: ç»˜å›¾æ•°æ®")

    tracemalloc.stop()
    print(f"\nğŸ¯ CEC2022åŸºå‡†æµ‹è¯•å®Œæˆ!")
    print(f"\nğŸ’¡ æç¤º: ä¸‹æ¬¡å¯ä»¥ä½¿ç”¨ main(load_existing_data=True) ç›´æ¥åŠ è½½æ•°æ®è¿›è¡Œåˆ†æ")

def load_and_plot(run_id=None):
    """ä»…åŠ è½½æ•°æ®å¹¶ç”Ÿæˆå›¾è¡¨ï¼Œä¸é‡æ–°è¿è¡Œå®éªŒ

    Args:
        run_id: æŒ‡å®šè¦åŠ è½½çš„å®éªŒIDï¼Œå¦‚æœä¸ºNoneåˆ™åŠ è½½æœ€æ–°çš„
    """
    main(load_existing_data=True, run_id=run_id)

def run_new_experiment():
    """è¿è¡Œæ–°çš„å®éªŒ"""
    main(load_existing_data=False)

def list_available_experiments():
    """åˆ—å‡ºå¯ç”¨çš„å®éªŒæ•°æ®"""
    data_manager = DataManager()
    data_manager.list_available_data()

if __name__ == "__main__":
    import sys

    if len(sys.argv) > 1:
        command = sys.argv[1].lower()

        if command == "load":
            # åŠ è½½å·²æœ‰æ•°æ®è¿›è¡Œåˆ†æ
            run_id = sys.argv[2] if len(sys.argv) > 2 else None
            print(f"ğŸ”„ åŠ è½½å·²æœ‰æ•°æ®è¿›è¡Œåˆ†æ...")
            load_and_plot(run_id)
        elif command == "list":
            # åˆ—å‡ºå¯ç”¨çš„å®éªŒæ•°æ®
            print(f"ğŸ“‹ åˆ—å‡ºå¯ç”¨çš„å®éªŒæ•°æ®...")
            list_available_experiments()
        elif command == "run":
            # è¿è¡Œæ–°å®éªŒ
            print(f"ğŸš€ å¼€å§‹æ–°çš„å®éªŒ...")
            run_new_experiment()
        else:
            print(f"âŒ æœªçŸ¥å‘½ä»¤: {command}")
            print(f"å¯ç”¨å‘½ä»¤:")
            print(f"  python cec2022_benchmark.py run    # è¿è¡Œæ–°å®éªŒ")
            print(f"  python cec2022_benchmark.py load   # åŠ è½½æœ€æ–°æ•°æ®è¿›è¡Œåˆ†æ")
            print(f"  python cec2022_benchmark.py load <run_id>  # åŠ è½½æŒ‡å®šæ•°æ®")
            print(f"  python cec2022_benchmark.py list   # åˆ—å‡ºå¯ç”¨æ•°æ®")
    else:
        # é»˜è®¤è¿è¡Œæ–°å®éªŒ
        print(f"ğŸš€ å¼€å§‹æ–°çš„å®éªŒ...")
        run_new_experiment()
